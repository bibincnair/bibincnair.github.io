---
layout: post
title: Paper Reading - Attention is all you need.
---

### Overview

### Core ideas

#### Transformer blocks

#### Attention heads

#### Positional encoding

### Why Transformer over Recurrent architectures
