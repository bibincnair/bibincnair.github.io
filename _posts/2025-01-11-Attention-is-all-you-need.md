---
layout: post
title: Paper Reading - Attention is all you need.
tags: [nlp, transformer, attention]
---

### Overview

### Core ideas

#### Transformer blocks

#### Attention heads

#### Positional encoding

### Why Transformer over Recurrent architectures
